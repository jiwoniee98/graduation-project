{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1uy0QDoD3-fI6UfM_ROprNBL-QptViFSv",
      "authorship_tag": "ABX9TyNbELBa1Nk+H0j75AyEICN4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoniee98/graduation-project/blob/master/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtYlPueRkx9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWoIEdY0sbrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " cd Mecab-ko-for-Google-Colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9MY3XbZsb7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLEvNpQ62m9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBtfY5zt4Agk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data_utils import Vocabulary\n",
        "from data_utils import load_data_interactive\n",
        "\n",
        "from data_loader import prepare_sequence, prepare_char_sequence, prepare_lex_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "#from Seq2Seq_test import Seq2Seq\n",
        "from data_loader import get_loader\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lDOx2uiugZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vocab_path='vocab_ko_NER.pkl'\n",
        "char_vocab_path='char_vocab_ko_NER.pkl'\n",
        "pos_vocab_path='pos_vocab_ko_NER.pkl'\n",
        "lex_dict_path='lex_dict.pkl'\n",
        "model_load_path='cnn_bilstm_tagger-179-400_f1_0.8739_maxf1_0.8739_100_200_2.pkl'\n",
        "num_layers=2\n",
        "embed_size=100\n",
        "hidden_size=200 \n",
        "gpu_index=0\n",
        "\n",
        "predict_NER_dict = {0: '<PAD>',\n",
        "                    1: '<START>',\n",
        "                    2: '<STOP>',\n",
        "                    3: 'B_LC',\n",
        "                    4: 'B_DT',\n",
        "                    5: 'B_OG',\n",
        "                    6: 'B_TI',\n",
        "                    7: 'B_PS',\n",
        "                    8: 'I',\n",
        "                    9: 'O'}\n",
        "\n",
        "NER_idx_dic = {'<unk>': 0, 'LC': 1, 'DT': 2, 'OG': 3, 'TI': 4, 'PS': 5}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBvuj39Cuiya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def to_np(x):\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "def to_var(x, volatile=False):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda(gpu_index)\n",
        "    return Variable(x, volatile=volatile)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJgnA4fjulBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply word2vec\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "from gensim.models import word2vec\n",
        "pretrained_word2vec_file = 'ko_word2vec_' + str(embed_size) + '.model'\n",
        "wv_model_ko = word2vec.Word2Vec.load(pretrained_word2vec_file)\n",
        "word2vec_matrix = wv_model_ko.wv.vectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcZuCJlLyr7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# build vocab\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "print(\"len(vocab): \",len(vocab))\n",
        "print(\"word2vec_matrix: \",np.shape(word2vec_matrix))\n",
        "with open(char_vocab_path, 'rb') as f:\n",
        "    char_vocab = pickle.load(f)\n",
        "with open(pos_vocab_path, 'rb') as f:\n",
        "    pos_vocab = pickle.load(f)\n",
        "with open(lex_dict_path, 'rb') as f:\n",
        "    lex_dict = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLG7Mi8eCnwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "# PackedSequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, char_vocab_size, pos_vocab_size, lex_ner_size, hidden_size, num_layers, embed_size,\n",
        "                 word2vec, num_classes):  # kernel_num=128, kernel_sizes=[2,3,4],\n",
        "        # 항상 torch.nn.Module 상속받고 시작\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        kernel_size = 1  # 커널\n",
        "        # 입력채널\n",
        "        channel_input_word = 1\n",
        "        channel_input_lexicon = 1\n",
        "        kernel_num = 128\n",
        "        kernel_sizes = [2, 3, 4, 5]\n",
        "        channel_output = kernel_num\n",
        "\n",
        "        if word2vec is not None:\n",
        "            self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "            self.embed.weight = torch.nn.parameter.Parameter(torch.Tensor(word2vec))\n",
        "            self.embed.weight.requires_grad = False\n",
        "\n",
        "            self.trainable_embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "            self.trainable_embed.weight = torch.nn.parameter.Parameter(torch.Tensor(word2vec))\n",
        "\n",
        "\n",
        "        else:\n",
        "            self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "\n",
        "        self.char_embed = nn.Embedding(char_vocab_size, embed_size, padding_idx=0)\n",
        "        self.pos_embed = nn.Embedding(pos_vocab_size, embed_size, padding_idx=0)\n",
        "        self.convs1 = nn.ModuleList(\n",
        "            [nn.Conv2d(channel_input_word, channel_output, (kernel_size, embed_size)) for kernel_size in kernel_sizes])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        #self.fc1 = nn.Linear(2 * hidden_size, num_classes)\n",
        "\n",
        "    # forward 함수 : 주어진 input을 init에서 선언한 모듈에 입력, output\n",
        "    def forward(self, x, x_char, x_pos, x_lex_embedding, lengths):\n",
        "\n",
        "        x_word_embedding = self.embed(x)  # (batch,words,word_embedding)\n",
        "        trainable_x_word_embedding = self.trainable_embed(x)\n",
        "\n",
        "        char_output = []\n",
        "        for i in range(x_char.size(1)):\n",
        "            x_char_embedding = self.char_embed(x_char[:, i]).unsqueeze(1)  # (batch,channel_input,words,word_embedding)\n",
        "\n",
        "            h_convs1 = [F.relu(conv(x_char_embedding)).squeeze(3) for conv in self.convs1]\n",
        "            h_pools1 = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in\n",
        "                        h_convs1]  # [(batch,channel_out), ...]*len(kernel_sizes)\n",
        "            h_pools1 = torch.cat(h_pools1, 1)  # 리스트에 있는걸 쌓아서 Tensor로\n",
        "            h_pools1 = self.dropout(h_pools1)\n",
        "            out = h_pools1.unsqueeze(1)\n",
        "            char_output.append(out)\n",
        "\n",
        "        char_output = torch.cat(char_output, 1)  # 단어 단위끼리 붙이고\n",
        "        x_pos_embedding = self.pos_embed(x_pos)\n",
        "        enhanced_embedding = torch.cat((char_output, x_word_embedding, trainable_x_word_embedding, x_pos_embedding), 2)  # 임베딩 차원(2)으로 붙이고\n",
        "        enhanced_embedding = self.dropout(enhanced_embedding)\n",
        "        enhanced_embedding = torch.cat((enhanced_embedding, x_lex_embedding), 2)\n",
        "\n",
        "        packed = pack_padded_sequence(enhanced_embedding, lengths, batch_first=True)\n",
        "\n",
        "        return packed\n",
        "\n",
        "    def sample(self, x, x_char, x_pos, x_lex_embedding, lengths):\n",
        "        x_word_embedding = self.embed(x)  # (batch,words,word_embedding)\n",
        "        trainable_x_word_embedding = self.trainable_embed(x)\n",
        "\n",
        "        char_output = []\n",
        "        for i in range(x_char.size(1)):\n",
        "            x_char_embedding = self.char_embed(x_char[:, i]).unsqueeze(1)  # (batch,channel_input,words,word_embedding)\n",
        "\n",
        "            h_convs1 = [F.relu(conv(x_char_embedding)).squeeze(3) for conv in self.convs1]\n",
        "            h_pools1 = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in\n",
        "                        h_convs1]\n",
        "            h_pools1 = torch.cat(h_pools1, 1)  # 리스트에 있는걸 쌓아서 Tensor로\n",
        "            h_pools1 = self.dropout(h_pools1)\n",
        "            out = h_pools1.unsqueeze(1)  # 단어단위 고려\n",
        "            char_output.append(out)\n",
        "\n",
        "        char_output = torch.cat(char_output, 1)  # 단어 단위끼리 붙이고\n",
        "\n",
        "        x_pos_embedding = self.pos_embed(x_pos)\n",
        "\n",
        "        enhanced_embedding = torch.cat((char_output, x_word_embedding, trainable_x_word_embedding, x_pos_embedding),\n",
        "                                       2)  # 임베딩 차원(2)으로 붙이고\n",
        "        enhanced_embedding = self.dropout(enhanced_embedding)\n",
        "        enhanced_embedding = torch.cat((enhanced_embedding, x_lex_embedding), 2)\n",
        "\n",
        "        return enhanced_embedding\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, encoder, lex_ner_size, hidden_size, num_layers, embed_size, word2vec, num_classes):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        kernel_num = 128\n",
        "        kernel_sizes = [2, 3, 4, 5]\n",
        "        channel_output = kernel_num\n",
        "\n",
        "        if word2vec is not None:\n",
        "            # BiLSTM\n",
        "            self.lstm = nn.LSTM((channel_output * len(kernel_sizes) + 2 * embed_size + embed_size + lex_ner_size),\n",
        "                                hidden_size, num_layers, dropout=0.6, batch_first=True, bidirectional=True)\n",
        "        else:\n",
        "            self.lstm = nn.LSTM((channel_output * len(kernel_sizes) + embed_size + embed_size + lex_ner_size),\n",
        "                                hidden_size, num_layers, dropout=0.6, batch_first=True, bidirectional=True)\n",
        "\n",
        "       #self.encdoer.fc1 = nn.Linear(2 * hidden_size, num_classes)\n",
        "        self.fc1 = nn.Linear(2 * hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, packed):\n",
        "        output_word, state_word = self.lstm(packed)\n",
        "        return output_word\n",
        "\n",
        "    def sample(self, enhanced_embedding):\n",
        "        output_word, state_word = self.lstm(enhanced_embedding)\n",
        "        return output_word\n",
        "\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        #self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, output_word):\n",
        "        return self.decoder.fc1(output_word[0])\n",
        "\n",
        "    def sample(self, output_word):\n",
        "        return self.decoder.fc1(output_word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DICqmJ2x2c5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# build models\n",
        "seq2seq_encoder = Encoder(vocab_size=len(vocab),\n",
        "                          char_vocab_size=len(char_vocab),\n",
        "                          pos_vocab_size=len(pos_vocab),\n",
        "                          lex_ner_size=len(NER_idx_dic),\n",
        "                          embed_size=embed_size,\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          word2vec=word2vec_matrix,\n",
        "                          num_classes=10)\n",
        "seq2seq_decoder = Decoder(seq2seq_encoder,\n",
        "                          lex_ner_size=len(NER_idx_dic),\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          embed_size=embed_size,\n",
        "                          word2vec=word2vec_matrix,\n",
        "\t\t\t\t\t\t  num_classes=10)\n",
        "seq2seq_model = Seq2Seq(decoder=seq2seq_decoder)\n",
        "\n",
        "\n",
        "# infernce를 위해 모델 저장하고 불러오기\n",
        "# seq2seq_model.load_state_dict(torch.load(model_load_path))\n",
        "torch.save(seq2seq_model.state_dict(), model_load_path)\n",
        "seq2seq_model.load_state_dict(torch.load(model_load_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    seq2seq_model.cuda(gpu_index)\n",
        "\n",
        "\n",
        "# 추론 전 드롭아웃 및 배치 정규화를 평가모드로 설정\n",
        "seq2seq_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3dcQyyB5bbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(x_text_batch, x_pos_batch, x_split_batch):\n",
        "   # print(\"1\", x_text_batch)\n",
        "   # print(\"2\", x_pos_batch)\n",
        "   # print(\"3\", x_split_batch)\n",
        "    x_text_char_item = []\n",
        "    for x_word in x_text_batch[0]:\n",
        "        x_char_item = []\n",
        "        for x_char in x_word:\n",
        "            x_char_item.append(x_char)\n",
        "        x_text_char_item.append(x_char_item)\n",
        "    x_text_char_batch = [x_text_char_item]\n",
        "    print(\"4\", x_split_batch)\n",
        "\n",
        "    x_idx_item = prepare_sequence(x_text_batch[0], vocab.word2idx)\n",
        "    print(\"5\",  x_idx_item)\n",
        "    x_idx_char_item = prepare_char_sequence(x_text_char_batch[0], char_vocab.word2idx)\n",
        "    print(\"6\",  x_idx_char_item)\n",
        "    x_pos_item = prepare_sequence(x_pos_batch[0], pos_vocab.word2idx)\n",
        "    print(\"7\",  x_pos_item)\n",
        "    x_lex_item = prepare_lex_sequence(x_text_batch[0], lex_dict)\n",
        "    print(\"8\",  x_lex_item)\n",
        "\n",
        "    x_idx_batch = [x_idx_item]\n",
        "    x_idx_char_batch = [x_idx_char_item]\n",
        "    x_pos_batch = [x_pos_item]\n",
        "    x_lex_batch = [x_lex_item]\n",
        "\n",
        "\n",
        "    max_word_len = int(np.amax([len(word_tokens) for word_tokens in x_idx_batch])) # ToDo: usually, np.mean can be applied\n",
        "    batch_size = len(x_idx_batch)\n",
        "    batch_words_len = [len(word_tokens) for word_tokens in x_idx_batch]\n",
        "    batch_words_len = np.array(batch_words_len)\n",
        "\n",
        "    # Padding procedure (word)\n",
        "    padded_word_tokens_matrix = np.zeros((batch_size, max_word_len), dtype=np.int64)\n",
        "    for i in range(padded_word_tokens_matrix.shape[0]):\n",
        "        for j in range(padded_word_tokens_matrix.shape[1]):\n",
        "            try:\n",
        "                padded_word_tokens_matrix[i, j] = x_idx_batch[i][j]\n",
        "            except IndexError:\n",
        "                pass\n",
        "\n",
        "    max_char_len = int(np.amax([len(char_tokens) for word_tokens in x_idx_char_batch for char_tokens in word_tokens]))\n",
        "    if max_char_len < 5: # size of maximum filter of CNN\n",
        "        max_char_len = 5\n",
        "        \n",
        "    # Padding procedure (char)\n",
        "    padded_char_tokens_matrix = np.zeros((batch_size, max_word_len, max_char_len), dtype=np.int64)\n",
        "    for i in range(padded_char_tokens_matrix.shape[0]):\n",
        "        for j in range(padded_char_tokens_matrix.shape[1]):\n",
        "            for k in range(padded_char_tokens_matrix.shape[1]):\n",
        "                try:\n",
        "                    padded_char_tokens_matrix[i, j, k] = x_idx_char_batch[i][j][k]\n",
        "                except IndexError:\n",
        "                    pass\n",
        "\n",
        "    # Padding procedure (pos)\n",
        "    padded_pos_tokens_matrix = np.zeros((batch_size, max_word_len), dtype=np.int64)\n",
        "    for i in range(padded_pos_tokens_matrix.shape[0]):\n",
        "        for j in range(padded_pos_tokens_matrix.shape[1]):\n",
        "            try:\n",
        "                padded_pos_tokens_matrix[i, j] = x_pos_batch[i][j]\n",
        "            except IndexError:\n",
        "                pass\n",
        "\n",
        "    # Padding procedure (lex)\n",
        "    padded_lex_tokens_matrix = np.zeros((batch_size, max_word_len, len(NER_idx_dic)))\n",
        "    for i in range(padded_lex_tokens_matrix.shape[0]):\n",
        "        for j in range(padded_lex_tokens_matrix.shape[1]):\n",
        "            for k in range(padded_lex_tokens_matrix.shape[2]):\n",
        "                try:\n",
        "                    for x_lex in x_lex_batch[i][j]:\n",
        "                        k = NER_idx_dic[x_lex]\n",
        "                        padded_lex_tokens_matrix[i, j, k] = 1\n",
        "                except IndexError:\n",
        "                    pass\n",
        "\n",
        "                \n",
        "    x_text_batch = x_text_batch\n",
        "    x_split_batch = x_split_batch\n",
        "    padded_word_tokens_matrix = torch.from_numpy(padded_word_tokens_matrix)\n",
        "    padded_char_tokens_matrix = torch.from_numpy(padded_char_tokens_matrix)\n",
        "    padded_pos_tokens_matrix = torch.from_numpy(padded_pos_tokens_matrix)\n",
        "    padded_lex_tokens_matrix = torch.from_numpy(padded_lex_tokens_matrix).float()\n",
        "    lengths = batch_words_len\n",
        "\n",
        "    return x_text_batch, x_split_batch, padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths\n",
        "\n",
        "def parsing_seq2NER(argmax_predictions, x_text_batch):\n",
        "    predict_NER_list = []\n",
        "    predict_text_NER_result_batch = copy.deepcopy(x_text_batch[0]) #tuple ([],) -> return first list (batch_size == 1)\n",
        "    for argmax_prediction_seq in argmax_predictions:\n",
        "        #print(\"argmax_predictions\", argmax_predictions)\n",
        "        #print(\"argmax_prediction_seq\", argmax_prediction_seq)\n",
        "        predict_NER = []\n",
        "        NER_B_flag = None # stop B\n",
        "        prev_NER_token = None\n",
        "        for i, argmax_prediction in enumerate(argmax_prediction_seq):\n",
        "                now_NER_token = predict_NER_dict[argmax_prediction.cpu().data.numpy()[0]]\n",
        "                predict_NER.append(now_NER_token)\n",
        "                ###\n",
        "                #print(\"1\", predict_NER)\n",
        "                #print(\"2\", now_NER_token)\n",
        "                if now_NER_token in ['B_LC', 'B_DT', 'B_OG', 'B_TI', 'B_PS'] and NER_B_flag is None: # O B_LC\n",
        "                    NER_B_flag = now_NER_token # start B\n",
        "                    predict_text_NER_result_batch[i] = '<'+predict_text_NER_result_batch[i]\n",
        "                    prev_NER_token = now_NER_token\n",
        "                    if i == len(argmax_prediction_seq)-1:\n",
        "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i]+':'+now_NER_token[-2:]+'>'\n",
        "\n",
        "                elif now_NER_token in ['B_LC', 'B_DT', 'B_OG', 'B_TI', 'B_PS'] and NER_B_flag is not None: # O B_LC B_DT\n",
        "                    predict_text_NER_result_batch[i-1] = predict_text_NER_result_batch[i-1]+':'+prev_NER_token[-2:]+'>'\n",
        "                    predict_text_NER_result_batch[i] = '<' + predict_text_NER_result_batch[i]\n",
        "                    prev_NER_token = now_NER_token\n",
        "                    if i == len(argmax_prediction_seq)-1:\n",
        "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i]+':'+now_NER_token[-2:]+'>'\n",
        "\n",
        "                elif now_NER_token in ['I'] and NER_B_flag is not None:\n",
        "                    if i == len(argmax_prediction_seq) - 1:\n",
        "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i] + ':' + NER_B_flag[-2:] + '>'\n",
        "\n",
        "                elif now_NER_token in ['O'] and NER_B_flag is not None: # O B_LC I O\n",
        "                    predict_text_NER_result_batch[i-1] = predict_text_NER_result_batch[i-1] + ':' + prev_NER_token[-2:] + '>'\n",
        "                    NER_B_flag = None # stop B\n",
        "                    prev_NER_token = now_NER_token\n",
        "\n",
        "        predict_NER_list.append(predict_NER)\n",
        "    return predict_NER_list, predict_text_NER_result_batch\n",
        "\n",
        "def generate_text_result(text_NER_result_batch, x_split_batch):\n",
        "    prev_x_split = 0 \n",
        "    text_string = ''\n",
        "    for i, x_split in enumerate(x_split_batch[0]):\n",
        "        if prev_x_split != x_split:\n",
        "            text_string = text_string+' '+text_NER_result_batch[i]\n",
        "            prev_x_split = x_split\n",
        "        else:\n",
        "            text_string = text_string +''+ text_NER_result_batch[i]\n",
        "            prev_x_split = x_split\n",
        "    return text_string\n",
        "\n",
        "\n",
        "def NER_print(input_str):\n",
        "    input_str.replace(\"  \", \"\")\n",
        "    input_str = input_str.strip()\n",
        "    #print(\"input_str : \", input_str)\n",
        "\n",
        "    x_text_batch, x_pos_batch, x_split_batch = load_data_interactive(input_str)\n",
        "    x_text_batch, x_split_batch, padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths = preprocessing(x_text_batch, x_pos_batch, x_split_batch)\n",
        "   # print(\"x_text_batch\", x_text_batch)\n",
        "   # print(\"x_pos_batc\",x_pos_batch)\n",
        "   # print(\"x_split_batch\",x_split_batch)\n",
        "   # print(\"padded_word_tokens_matrix\",padded_word_tokens_matrix)\n",
        "   # print(\"padded_char_tokens_matrix\",padded_char_tokens_matrix)\n",
        "   # print(\"padded_pos_tokens_matrix\",padded_pos_tokens_matrix)\n",
        "   # print(\"padded_lex_tokens_matrix\",padded_lex_tokens_matrix)\n",
        "   # print(\"lengths\",lengths)\n",
        "\n",
        "    # Test\n",
        "    argmax_labels_list = []\n",
        "    argmax_predictions_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        padded_word_tokens_matrix = to_var(padded_word_tokens_matrix)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        padded_char_tokens_matrix = to_var(padded_char_tokens_matrix)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        padded_pos_tokens_matrix = to_var(padded_pos_tokens_matrix)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        padded_lex_tokens_matrix = to_var(padded_lex_tokens_matrix)\n",
        "    #print(padded_word_tokens_matrix)\n",
        "    #print(padded_char_tokens_matrix)\n",
        "    #print(padded_pos_tokens_matrix)\n",
        "    #print(padded_lex_tokens_matrix)\n",
        "    \n",
        "    predictions_en = seq2seq_encoder.sample(padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths)\n",
        "    \n",
        "    predictions_de = seq2seq_decoder.sample(predictions_en)\n",
        "\n",
        "    predictions = seq2seq_model.sample(predictions_de)\n",
        "    \n",
        "    #predictions = seq2seq_model.sample(padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths)\n",
        "    max_predictions, argmax_predictions = predictions.max(2)\n",
        "\n",
        "    if len(argmax_predictions.size()) != len(\n",
        "        predictions.size()):  # Check that class dimension is reduced or not (API version issue, pytorch 0.1.12)\n",
        "        max_predictions, argmax_predictions = predictions.max(2, keepdim=True)\n",
        "\n",
        "    argmax_predictions_list.append(argmax_predictions)\n",
        "    \n",
        "    predict_NER_list, predict_text_NER_result_batch = parsing_seq2NER(argmax_predictions, x_text_batch)\n",
        "\n",
        "\n",
        "#     print(\"x_text: \",x_text_batch)\n",
        "#     print(\"NER_pred: \",predict_NER_list)\n",
        "#     print(\"predict_text_NER_result_batch: \",predict_text_NER_result_batch)\n",
        "#     print(\"x_split_batch: \",x_split_batch)\n",
        "    \n",
        "    \n",
        "    origin_text_string = generate_text_result(x_text_batch[0], x_split_batch)\n",
        "    predict_NER_text_string = generate_text_result(predict_text_NER_result_batch, x_split_batch)\n",
        "\n",
        "\n",
        "#     print(\"origin:  \",origin_text_string)\n",
        "#     print(\"predict: \",predict_NER_text_string)\n",
        "    print(\"output> \",predict_NER_text_string)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjH5zIz_5qqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델성능 f1 87.39\n",
        "while(True):\n",
        "    \n",
        "    input_str = input('input> ')\n",
        "    \n",
        "    if input_str == 'exit':\n",
        "        break\n",
        "    else:\n",
        "        NER_print(input_str)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}